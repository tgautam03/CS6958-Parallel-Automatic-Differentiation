{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "using Base\n",
    "\n",
    "using BenchmarkTools\n",
    "using Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse Mode Automatic Differentiation\n",
    "\n",
    "## Introduction\n",
    "Reverse Mode AD is a little different from Forward Mode AD. In reverse accumulation AD, the dependent variable to be differentiated is fixed and the derivative is computed with respect to each sub-expression recursively. In other words, the derivative of the outer functions is repeatedly substituted in the chain rule:\n",
    "\n",
    "$\\begin{aligned}\n",
    "\\frac{\\partial y}{\\partial x}   &= \\frac{\\partial y}{\\partial w_1} \\cdot \\frac{\\partial w_1}{\\partial x} \\\\\n",
    "                                &= \\bigg( \\frac{\\partial y}{\\partial w_2} \\cdot \\frac{\\partial w_2}{\\partial w_1} \\bigg) \\cdot \\frac{\\partial w_1}{\\partial x} \\\\\n",
    "                                &= \\cdots \\\\\n",
    "\\end{aligned}$\n",
    "\n",
    "Consider a function $f(x_1, x_2)=\\sin(x_1) + (x_1 \\cdot x_2)$. To get the derivatives with respect to $x_1$ and $x_2$, Reverse accumulation traverses the chain rule from outside to inside. The example function is scalar-valued, and thus there is only one seed for the derivative computation, and only one sweep of the computational graph is needed to calculate the (two-component) gradient. This is only half the work when compared to forward accumulation (this is only true if the output is scalar), but reverse accumulation requires the storage of the intermediate variables as well as the instructions that produced them in a data structure known as a \"tape\", which may consume significant memory if the computational graph is large. This can be mitigated to some extent by storing only a subset of the intermediate variables and then reconstructing the necessary work variables by repeating the evaluations, a technique known as rematerialization. Checkpointing is also used to save intermediary states.\n",
    "\n",
    "In this work, I'll build a simple Reverse AD which tracks the computation graph and then evaluates the derivatives. To do this, I'll define a `mutable struct Variable` that holds the value, parents and the chain rule to evaluate derivatives with respect to it's parents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Variable\n",
    "    value::Number       # Value of this Variable\n",
    "    parents::Vector     # Who created this Variable\n",
    "    chain_rules::Vector # Functions representing the chain rules\n",
    "\n",
    "    # Constructor for creating Input Nodes\n",
    "    function Variable(value::Number)\n",
    "        new(value, [], [])\n",
    "    end\n",
    "\n",
    "    # Constructor for creating Intermediate/Output Nodes\n",
    "    function Variable(value::Number, parents::Vector{Variable}, chain_rules::Vector)\n",
    "        new(value, parents, chain_rules)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I just have to define the operations of `Variable` to store the parents and define the chain rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addition Operaton on Scalars\n",
    "function Base.:+(var1::Variable, var2::Variable)\n",
    "    # Performing Addition\n",
    "    value = var1.value + var2.value\n",
    "\n",
    "    # Local Gradients Computations\n",
    "    global_dvar1 = global_grad::Variable -> global_grad * 1\n",
    "    global_dvar2 = global_grad::Variable -> global_grad * 1\n",
    "\n",
    "    return Variable(value, [var1, var2], [global_dvar1, global_dvar2])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplication Operaton on Scalars\n",
    "function Base.:*(var1::Variable, var2::Variable)\n",
    "    # Performing Multiplication\n",
    "    value = var1.value * var2.value\n",
    "\n",
    "    # Local Gradients Computations\n",
    "    global_dvar1 = global_grad::Variable -> global_grad * var2\n",
    "    global_dvar2 = global_grad::Variable -> global_grad * var1\n",
    "\n",
    "    return Variable(value, [var1, var2], [global_dvar1, global_dvar2])\n",
    "end\n",
    "\n",
    "# Multiplication Operaton on Scalars\n",
    "function Base.:*(var1::Variable, var2::Number)\n",
    "    # Performing Multiplication\n",
    "    value = var1.value * var2\n",
    "\n",
    "    # Local Gradients Computations\n",
    "    global_dvar1 = global_grad::Variable -> global_grad * var2\n",
    "\n",
    "    return Variable(value, [var1], [global_dvar1])\n",
    "end\n",
    "\n",
    "# Multiplication Operaton on Scalars\n",
    "function Base.:*(var1::Number, var2::Variable)\n",
    "    # Performing Multiplication\n",
    "    value = var1 * var2.value\n",
    "\n",
    "    # Local Gradients Computations\n",
    "    global_dvar2 = global_grad::Variable -> global_grad * var1\n",
    "\n",
    "    return Variable(value, [var2], [global_dvar2])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Base.:^(var1::Variable, pow::Int)\n",
    "    # Performing power\n",
    "    value = var1.value^pow\n",
    "\n",
    "    # Local Gradients Computations\n",
    "    global_dvar1 = global_grad::Variable -> global_grad * pow * var1^(pow-1)\n",
    "\n",
    "    return Variable(value, [var1], [global_dvar1])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Base.:sin(var1::Variable)\n",
    "    # Performing Addition\n",
    "    value = sin(var1.value)\n",
    "\n",
    "    # Local Gradients Computations\n",
    "    global_dvar1 = global_grad::Variable -> global_grad * cos(var1)\n",
    "\n",
    "    return Variable(value, [var1], [global_dvar1])\n",
    "end\n",
    "\n",
    "function Base.:cos(var1::Variable)\n",
    "    # Performing Addition\n",
    "    value = cos(var1.value)\n",
    "\n",
    "    # Local Gradients Computations\n",
    "    global_dvar1 = global_grad::Variable -> global_grad * -1 * sin(var1)\n",
    "\n",
    "    return Variable(value, [var1], [global_dvar1])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the computation graph is defined, we have to move backwards and collect the gradients. I've done that by defining the function `autograd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "autograd (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Uses compute() to get gradients and store them in a dict\n",
    "function autograd(T::Variable)\n",
    "\tgradients = Dict() # Dict to hold grads of T wrt all creators\n",
    "\n",
    "    # Computes the global gradient WRT node\n",
    "    function compute(node::Variable, global_grad::Variable)\n",
    "        for (parent, chain_rule) in zip(node.parents, node.chain_rules)\n",
    "            # Chain Rule\n",
    "            new_global_grad = chain_rule(global_grad)\n",
    "\n",
    "            # Checking if grad present or not\n",
    "            if haskey(gradients, parent)\n",
    "                gradients[parent] += new_global_grad\n",
    "            else\n",
    "                gradients[parent] = new_global_grad\n",
    "            end\n",
    "\n",
    "            # Recusive call\n",
    "            compute(parent, new_global_grad)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "\t# Output Node\n",
    "\tdT_dT = Variable(1)\n",
    "\tcompute(T, dT_dT)\n",
    "\n",
    "\treturn gradients\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Differentiating a simple equation\n",
    "Let's differentiate $f(x)=x^2+2x$ at $x=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Value: 8\n",
      "Function Gradient: 6\n"
     ]
    }
   ],
   "source": [
    "# Defining Function\n",
    "h(x) = x^2 + 2*x\n",
    "\n",
    "# Defining x\n",
    "x = Variable(2)\n",
    "\n",
    "# Forward Pass\n",
    "y = h(x)\n",
    "\n",
    "# Backward Pass\n",
    "dy_dx = autograd(y)[x]\n",
    "\n",
    "println(\"Function Value: \", y.value)\n",
    "println(\"Function Gradient: \", dy_dx.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For parallelisation in reverse mode AD, we can only rely on the parallelisation in the computation graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Reverse Mode Automatic Differentiation: Stencil Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parallel_stencil (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stencil Computation\n",
    "function stencil(a,b,c) \n",
    "    return 2*a * 3*b * 4*c\n",
    "end\n",
    "\n",
    "# Parallel Stencil Computation\n",
    "function parallel_stencil(arr_in, vals_out, grads_out)\n",
    "    Threads.@threads for i=2:size(arr_in)[1]-1\n",
    "        a = Variable(arr_in[i-1])\n",
    "        b = Variable(arr_in[i])\n",
    "        c = Variable(arr_in[i+1])\n",
    "        # Forward Pass\n",
    "        output = stencil(a, b, c)\n",
    "        # Derivative\n",
    "        d_output = autograd(output)\n",
    "\n",
    "        vals_out[i-1] = output.value\n",
    "        grads_out[i-1, :] = [d_output[a].value, d_output[b].value, d_output[c].value]\n",
    "    end\n",
    "\n",
    "    return vals_out, grads_out\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Threads: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel Evaluation Time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  683.638 ms (16698654 allocations: 588.96 MiB)\n"
     ]
    }
   ],
   "source": [
    "# Number of available threads\n",
    "println(\"Number of Threads: \", Base.Threads.nthreads())\n",
    "\n",
    "# Computation\n",
    "Random.seed!(123)\n",
    "arr_in = rand(100000,1)\n",
    "vals_out = zeros(size(arr_in)[1]-2,1)\n",
    "grads_out = zeros(size(arr_in)[1]-2,3)\n",
    "\n",
    "vals_out, grads_out = parallel_stencil(arr_in, vals_out, grads_out) \n",
    "\n",
    "println(\"Parallel Evaluation Time\")\n",
    "@btime _, _ = parallel_stencil(arr_in, vals_out, grads_out); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential_stencil (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sequential Stencil Computation\n",
    "function sequential_stencil(arr_in, vals_out, grads_out)\n",
    "    for i=2:size(arr_in)[1]-1\n",
    "        a = Variable(arr_in[i-1])\n",
    "        b = Variable(arr_in[i])\n",
    "        c = Variable(arr_in[i+1])\n",
    "        # Forward Pass\n",
    "        output = stencil(a, b, c)\n",
    "        # Derivative\n",
    "        d_output = autograd(output)\n",
    "\n",
    "        vals_out[i-1] = output.value\n",
    "        grads_out[i-1, :] = [d_output[a].value, d_output[b].value, d_output[c].value]\n",
    "    end\n",
    "\n",
    "    return vals_out, grads_out\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Evaluation Time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  680.299 ms (16698645 allocations: 588.96 MiB)\n"
     ]
    }
   ],
   "source": [
    "println(\"Sequential Evaluation Time\")\n",
    "@btime _, _ = sequential_stencil(arr_in, vals_out, grads_out); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the stencil computation a little more complicated with other functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "complex_parallel_stencil (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stencil Computation\n",
    "function complex_stencil(a,b,c) \n",
    "    return sin(2*a) * cos(3*b) * 4*c\n",
    "end\n",
    "\n",
    "# Parallel Stencil Computation\n",
    "function complex_parallel_stencil(arr_in, vals_out, grads_out)\n",
    "    Threads.@threads for i=2:size(arr_in)[1]-1\n",
    "        a = Variable(arr_in[i-1])\n",
    "        b = Variable(arr_in[i])\n",
    "        c = Variable(arr_in[i+1])\n",
    "        # Forward Pass\n",
    "        output = complex_stencil(a, b, c)\n",
    "        # Derivative\n",
    "        d_output = autograd(output)\n",
    "\n",
    "        vals_out[i-1] = output.value\n",
    "        grads_out[i-1, :] = [d_output[a].value, d_output[b].value, d_output[c].value]\n",
    "    end\n",
    "\n",
    "    return vals_out, grads_out\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel Evaluation Time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.011 s (21898550 allocations: 773.59 MiB)\n"
     ]
    }
   ],
   "source": [
    "# Computation\n",
    "Random.seed!(123)\n",
    "arr_in = rand(100000,1)\n",
    "vals_out = zeros(size(arr_in)[1]-2,1)\n",
    "grads_out = zeros(size(arr_in)[1]-2,3)\n",
    "\n",
    "vals_out, grads_out = complex_parallel_stencil(arr_in, vals_out, grads_out) \n",
    "\n",
    "println(\"Parallel Evaluation Time\")\n",
    "@btime _, _ = complex_parallel_stencil(arr_in, vals_out, grads_out); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.4",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
