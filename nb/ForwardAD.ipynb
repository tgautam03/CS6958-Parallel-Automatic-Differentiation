{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "using Base\n",
    "using StaticArrays\n",
    "\n",
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Mode Automatic Differentiation\n",
    "\n",
    "**Things to do**\n",
    "\n",
    "- `Dual` Number Intro\n",
    "- `Dual` Number Example\n",
    "- Why we need `MultiDual`\n",
    "- `MultiDual` Intro\n",
    "- `MultiDual` Example\n",
    "- Stencil Computation: Where Parallelism comes in...\n",
    "\n",
    "## Introduction\n",
    "The core idea behind Forward Mode Automatic Differentiation comes from **Complex Step Finite Difference** where the value and the chance is stored separately, i.e. *Dual Numbers*. From Taylor Series we know that $f(x+h) \\approx f(x)+hf^{'}(x)$. This statement gives us a way to represent a function by it's value and derivative:\n",
    "\n",
    "$$\n",
    "f(x)\\rightarrow f(x)+\\epsilon f^{'}(x) \\tag{1}\n",
    "$$\n",
    "\n",
    "To enable this idea, we can define a custom `struct` which stores both value and the derivative. This `Dual` number is a multidimensional number where senstivity (or derivative) of the function is propagated alongside the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Dual{T}\n",
    "    # Value\n",
    "    val::T\n",
    "    # Derivative\n",
    "    grad::T\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Equation 1, let's define two function $f(x)$ and $g(x)$:\n",
    "\n",
    "$$\n",
    "f(x)\\rightarrow f(x)+\\epsilon f^{'}(x) \\tag{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "g(x)\\rightarrow g(x)+\\epsilon g^{'}(x) \\tag{3}\n",
    "$$\n",
    "\n",
    "Now using the two above defined equations, we can perform several arithmatic operations on them. Adding Equation 2 and 3 gives \n",
    "\n",
    "$$\n",
    "(f+g)(x)\\rightarrow \\big(f(x)+g(x)\\big)+\\epsilon \\big[f^{'}(x)+g^{'}(x)\\big]\n",
    "$$\n",
    "\n",
    "This can be coded easily by overriding `+` operator such that it collects gradients as well as the value inside the `Dual` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Base.:+(f::Dual{T}, g::Dual{T}) where {T}\n",
    "    return Dual{T}(f.val+g.val, f.grad+g.grad)\n",
    "end\n",
    "\n",
    "function Base.:+(f::Number, g::Dual{T}) where {T}\n",
    "    return Dual{T}(f+g.val, g.grad)\n",
    "end\n",
    "\n",
    "function Base.:+(f::Dual{T}, g::Number) where {T}\n",
    "    return Dual{N,T}(f.val*g, f.grad)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar rules can be applied for all the common operators.\n",
    "\n",
    "- **Subtraction Rule**\n",
    "\n",
    "    $$(f-g)(x)\\rightarrow f(x)-g(x)+\\epsilon \\big[f^{'}(x)-g^{'}(x)\\big]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Base.:-(f::Dual{T}, g::Dual{T}) where {T}\n",
    "    return Dual{T}(f.val-g.val, f.grad-g.grad)\n",
    "end\n",
    "\n",
    "function Base.:-(f::Number, g::Dual{T}) where {T}\n",
    "    return Dual{T}(f-g.val, g.grad)\n",
    "end\n",
    "\n",
    "function Base.:-(f::Dual{T}, g::Number) where {T}\n",
    "    return Dual{T}(f.val-g, f.grad)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Multiplication Rule**\n",
    "\n",
    "    $$(f \\cdot g)(x)\\rightarrow f(x)g(x)+\\epsilon \\big[f(x)g^{'}(x)+f^{'}(x)g(x)\\big]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Base.:*(f::Dual{T}, g::Dual{T}) where {T}\n",
    "    return Dual{T}(f.val*g.val, f.val*g.grad+f.grad*g.val)\n",
    "end\n",
    "\n",
    "function Base.:*(f::Number, g::Dual{T}) where {T}\n",
    "    return Dual{T}(f*g.val, f*g.grad)\n",
    "end\n",
    "\n",
    "function Base.:*(f::Dual{T}, g::Number) where {T}\n",
    "    return Dual{T}(f.val*g, f.grad*g)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Division Rule**\n",
    "\n",
    "    $$\\bigg(\\frac{f}{g}\\bigg)(x)\\rightarrow \\frac{f(x)}{g(x)}+\\epsilon \\bigg[\\frac{f^{'}(x)g(x)-f(x)g^{'}(x)}{g(x)^2}\\bigg]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Base.:/(f::Dual{T}, g::Dual{T}) where {T}\n",
    "    return Dual{T}(f.val/g.val, (f.grad*g.val-f.val*g.grad)/g.val^2)\n",
    "end\n",
    "\n",
    "function Base.:/(f::Number, g::Dual{T}) where {T}\n",
    "    return Dual{T}(f/g.val, (-f*g.grad)/g.val^2)\n",
    "end\n",
    "\n",
    "function Base.:/(f::Dual{T}, g::Number) where {T}\n",
    "    return Dual{T}(f.val/g, (f.grad*g)/g^2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Exponential Operator**\n",
    "\n",
    "    We know that $x^3$ basically means $((x\\times x)\\times x)$, so we can exploit the **multiplication rule** to define this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Base.:^(f::Dual{T}, g::Number) where {T}\n",
    "    return Base.power_by_squaring(f, g)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Differentiating a simple equation\n",
    "Suppose we have a function $f(x)=x^2+2x$. From chain rule, we can write $\\frac{\\partial f}{\\partial x}=\\frac{\\partial f}{\\partial x} \\times \\frac{\\partial x}{\\partial x}$ where $\\frac{\\partial x}{\\partial x}=1$. Hence to get the *automatic differentiation* to supply $\\frac{\\partial f}{\\partial x}$, we need to define the input $x$ as a `Dual` number, i.e. `Dual(x, 1)`, where `x` is the value at which we want to compute gradient.\n",
    "\n",
    "Let's evaluate gradient at $x=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Value: 8\n",
      "Function Gradient: 6\n"
     ]
    }
   ],
   "source": [
    "# Defining Function\n",
    "h(x) = x^2 + 2*x\n",
    "\n",
    "# Defining x\n",
    "x = Dual(2,1)\n",
    "\n",
    "# Evaluating h(x)\n",
    "y = h(x)\n",
    "\n",
    "println(\"Function Value: \", y.val)\n",
    "println(\"Function Gradient: \", y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amazing thing is that due to Vector Registers in Modern CPUs, value and grad are evaluated in parallel. This is also known as Vectorisation and is very common in other libraries like NumPy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just Evaluating value of the Function\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.929 ns (0 allocations: 0 bytes)\n",
      "Evaluating Value and Derivative of the function\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.929 ns (0 allocations: 0 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Normal Function eval\n",
    "println(\"Just Evaluating value of the Function\")\n",
    "@btime _ = h(2);\n",
    "\n",
    "# Dual Function eval\n",
    "println(\"Evaluating Value and Derivative of the function\")\n",
    "@btime _ = h(Dual(2,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have functions $f(x, y) = x^2 + xy$ and $g(x, y) = y^3 + x$ and want to compute jacobian \n",
    "$J = \n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y}\\\\\n",
    "\\frac{\\partial g}{\\partial x} & \\frac{\\partial g}{\\partial y}\n",
    "\\end{bmatrix}\n",
    "$ at $x=3, y=4$.\n",
    "\n",
    "This can be done by evaluating all four elements separately as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2×2 Matrix{Float64}:\n",
      " 10.0   3.0\n",
      "  1.0  48.0"
     ]
    }
   ],
   "source": [
    "# Defining Functions\n",
    "f(x, y) = x^2 + x*y\n",
    "g(x, y) = y^3 + x\n",
    "\n",
    "# Getting Derivatives\n",
    "df_dx = f(Dual(3, 1), Dual(4, 0)).grad\n",
    "df_dy = f(Dual(3, 0), Dual(4, 1)).grad\n",
    "\n",
    "dg_dx = g(Dual(3, 1), Dual(4, 0)).grad\n",
    "dg_dy = g(Dual(3, 0), Dual(4, 1)).grad\n",
    "\n",
    "\n",
    "J = zeros(2, 2)\n",
    "J[1,1] = df_dx\n",
    "J[1,2] = df_dy\n",
    "J[2,1] = dg_dx\n",
    "J[2,2] = dg_dy\n",
    "\n",
    "println(\"Jacobian: \")\n",
    "show(stdout, \"text/plain\", J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, for $f(x_1, x_2, \\cdots, x_n)$, we'll have to do differentiation `n` times! Another problem here is that we are not able to utilise the Vectorisation (i.e. Jacobian Computation is not parallel). We know that \n",
    "$\\nabla f = \\left[\\begin{array}{ccc}\n",
    "\\dfrac{\\partial f(x,y)}{\\partial x} & \\dfrac{\\partial f(x,y)}{\\partial y}\n",
    "\\end{array}\\right]$, and if we want vectorisation we need something like `df = ff(X).grads where (X=[x y])` and `df=[ff(Dual(3, 1), Dual(4, 0)).grad, ff(Dual(3, 0), Dual(4, 1)).grad]`, in turn utilise the **vector instructions** to calculate each partial derivative in parallel.\n",
    "\n",
    "This can be done by storing the derivatives in a `Vector` type rather than a scalar `grad`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `MultiDual`\n",
    "As discussed above, to utilise vectorisation we can store gradients in a `SVector` type. This is a Static Vector which lives on the stack for fast access (this makes sense because the dimensions for most real life problems can't be horribly large). In `MultiDual`, `N` defines the number of variables in the function and `T` defines the type (`Int`, `Float`, etc. etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct MultiDual{N,T}\n",
    "\t# Value\n",
    "\tval::T\n",
    "\t# SVector is static vector which lives on the stack\n",
    "\tgrads::SVector{N,T} \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various rules can be defined using Taylor Series (like before), i.e. if $x$ is a vector then Taylor Series is defined as \n",
    "\n",
    "$$f(x+\\epsilon)=f(x)+\\epsilon \\nabla f(x)+O(\\epsilon)$$\n",
    "\n",
    "Only change is that $f^{'}$ is replaced by $\\nabla f$ and the same thing goes for various differentiation rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Addition Rule**\n",
    "    $$(f+g)(x)\\rightarrow f(x)+g(x)+\\epsilon \\big[\\nabla f(x)+\\nabla g(x)\\big]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Base.:+(f::MultiDual{N,T}, g::MultiDual{N,T}) where {N,T}\n",
    "    return MultiDual{N,T}(f.val+g.val, f.grads+g.grads)\n",
    "end\n",
    "\n",
    "function Base.:+(f::Number, g::MultiDual{N,T}) where {N,T}\n",
    "    return MultiDual{N,T}(f+g.val, g.grads)\n",
    "end\n",
    "\n",
    "function Base.:+(f::MultiDual{N,T}, g::Number) where {N,T}\n",
    "    return MultiDual{N,T}(f.val+g, f.grads)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Subtraction Rule**\n",
    "    $$(f-g)(x)\\rightarrow f(x)-g(x)+\\epsilon \\big[\\nabla f(x)-\\nabla g(x)\\big]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Base.:-(f::MultiDual{N,T}, g::MultiDual{N,T}) where {N,T}\n",
    "    return MultiDual{N,T}(f.val-g.val, f.grads-g.grads)\n",
    "end\n",
    "\n",
    "function Base.:-(f::Number, g::MultiDual{N,T}) where {N,T}\n",
    "    return MultiDual{N,T}(f-g.val, g.grads)\n",
    "end\n",
    "\n",
    "function Base.:-(f::MultiDual{N,T}, g::Number) where {N,T}\n",
    "    return MultiDual{N,T}(f.val-g, f.grads)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Multiplication Rule**\n",
    "    $$(f \\cdot g)(x)\\rightarrow f(x)g(x)+\\epsilon \\big[f(x)\\nabla g(x)+\\nabla f(x)g(x)\\big]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Base.:*(f::MultiDual{N,T}, g::MultiDual{N,T}) where {N,T}\n",
    "    return MultiDual{N,T}(f.val*g.val, f.val*g.grads+f.grads*g.val)\n",
    "end\n",
    "\n",
    "function Base.:*(f::Number, g::MultiDual{N,T}) where {N,T}\n",
    "    return MultiDual{N,T}(f*g.val, f*g.grads)\n",
    "end\n",
    "\n",
    "function Base.:*(f::MultiDual{N,T}, g::Number) where {N,T}\n",
    "    return MultiDual{N,T}(f.val*g, f.grads*g)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Division Rule**\n",
    "    $$(\\frac{f}{g})(x)\\rightarrow \\frac{f(x)}{g(x)}+\\epsilon \\bigg[\\frac{\\nabla f(x)g(x)-f(x)\\nabla g(x)}{g(x)^2}\\bigg]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Base.:/(f::MultiDual{N,T}, g::MultiDual{N,T}) where {N,T}\n",
    "    return MultiDual{N,T}(f.val/g.val, (f.grads*g.val-f.val*g.grads)/g.val^2)\n",
    "end\n",
    "\n",
    "function Base.:/(f::Number, g::MultiDual{N,T}) where {N,T}\n",
    "    return MultiDual{N,T}(f/g.val, (-f.val*g.grads)/g.val^2)\n",
    "end\n",
    "\n",
    "function Base.:/(f::MultiDual{N,T}, g::Number) where {N,T}\n",
    "    return MultiDual{N,T}(f.val/g, (f.grads*g.val)/g^2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Exponential operator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Base.:^(f::MultiDual{N,T}, g::Number) where {N,T}\n",
    "    return Base.power_by_squaring(f,g)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Efficient Jacobian\n",
    "Let's now compute the Jacobian using `MultiDual`. The two functions are \n",
    "\n",
    "$$f(x, y) = x^2 + xy$$ \n",
    "\n",
    "$$g(x, y) = y^3 + x$$\n",
    "\n",
    "The next step is to define `MultiDual` functions for `x` and `y`. \n",
    "- Let a function $xx(x,y)=x + 0 \\times y$. `MultiDual` `xx` can be written as `xx = MultiDual(x, SVector(1.,0.))`. `SVector(1.,0.)` is written because $\\frac{\\partial xx}{x} = 1$ and $\\frac{\\partial xx}{y} = 0$, hence these are set as the first and second elements of the vector.\n",
    "- Similar for $yy(x,y)=y + 0 \\times x$, `yy=MultiDual(y, SVector(0.,1.))`.\n",
    "\n",
    "Jacobian can then easily be calculated in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian: \n",
      "2×2 Matrix{Float64}:\n",
      " 10.0   3.0\n",
      "  1.0  48.0"
     ]
    }
   ],
   "source": [
    "f(x, y) = x^2 + x*y\n",
    "g(x, y) = y^3 + x\n",
    "\n",
    "# Jacobian at x=3 and y=4\n",
    "xx = MultiDual(3., SVector(1.,0.))\n",
    "yy = MultiDual(4., SVector(0.,1.))\n",
    "\n",
    "# Function and Jacobian computation\n",
    "f_ = f(xx, yy)\n",
    "g_ = g(xx, yy)\n",
    "\n",
    "df_dx, df_dy = f_.grads \n",
    "dg_dx, dg_dy = g_.grads \n",
    "\n",
    "J = zeros(2, 2)\n",
    "J[1,1] = df_dx\n",
    "J[1,2] = df_dy\n",
    "J[2,1] = dg_dx\n",
    "J[2,2] = dg_dy\n",
    "\n",
    "println(\"Jacobian: \")\n",
    "show(stdout, \"text/plain\", J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Forward Mode Automatic Differentiation: Stencil Computation\n",
    "So far the derivative computation has been parallelised. However, if the computations have a structure that can be further parallelised, it can be made even more efficient. To demonstrate this, let's consider an example of 3 point Stencil Computation. Taking an input array `in_arr` of length `n`, then the 3-point stencil computation has the following pseudocode:\n",
    "\n",
    "```\n",
    "for in_arr_index = 2 to n-1\n",
    "    out_arr[in_arr_index - 1] = a*in_arr[in_arr_index - 1] + b*in_arr[in_arr_index] + c*in_arr[in_arr_index + 1]\n",
    "end\n",
    "```\n",
    "\n",
    "In the above calculations, `a`, `b` and `c` are scalars and I've demonstrated using the multiplication and addition operations. However, that can be changed to encorporate any other operation. For instance, to make things a little more interesting, I'm replacing addition with multiplication as well. Parallelising this is just a matter of running the loop interations over multiple threads in parallel. As each thread will write to a unique location, there are no race conditions! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parallel_stencil (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stencil Computation\n",
    "function stencil(a,b,c) \n",
    "    return 2*a * 3*b * 4*c\n",
    "end\n",
    "\n",
    "# Parallel Stencil Computation over an array\n",
    "function parallel_stencil(arr_in, vals_out, grads_out)\n",
    "    Threads.@threads for i=2:size(arr_in)[1]-1\n",
    "        output = stencil(MultiDual(arr_in[i-1], SVector(1.,0.,0.)), MultiDual(arr_in[i], SVector(0.,1.,0.)), MultiDual(arr_in[i+1], SVector(0.,0.,1.)))\n",
    "        vals_out[i-1] = output.val\n",
    "        grads_out[i-1,:] = output.grads\n",
    "    end\n",
    "\n",
    "    return vals_out, grads_out\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Threads: 16\n",
      "Parallel Evaluation Time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  75.110 μs (98 allocations: 3.06 MiB)\n"
     ]
    }
   ],
   "source": [
    "# Number of available threads\n",
    "println(\"Number of Threads: \", Base.Threads.nthreads())\n",
    "\n",
    "# Computation\n",
    "arr_in = rand(100000,1)\n",
    "vals_out = zeros(size(arr_in)[1]-2,1)\n",
    "grads_out = zeros(size(arr_in)[1]-2,3)\n",
    "\n",
    "vals_out, grads_out = parallel_stencil(arr_in, vals_out, grads_out) \n",
    "\n",
    "println(\"Parallel Evaluation Time\")\n",
    "@btime _, _ = parallel_stencil(arr_in); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to ensure that we are actually getting a speedup, let's look at the sequential stencil computation for a very large array size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential_stencil (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sequential Stencil Computation over an array\n",
    "function sequential_stencil(arr_in, vals_out, grads_out)\n",
    "    for i=2:size(arr_in)[1]-1\n",
    "        output = stencil(MultiDual(arr_in[i-1], SVector(1.,0.,0.)), MultiDual(arr_in[i], SVector(0.,1.,0.)), MultiDual(arr_in[i+1], SVector(0.,0.,1.)))\n",
    "        vals_out[i-1] = output.val\n",
    "        grads_out[i-1,:] = output.grads\n",
    "    end\n",
    "\n",
    "    return vals_out, grads_out\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Evaluation Time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  283.019 μs (1 allocation: 32 bytes)\n"
     ]
    }
   ],
   "source": [
    "println(\"Sequential Evaluation Time\")\n",
    "@btime _, _ = sequential_stencil(arr_in, vals_out, grads_out); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform several modifications to this stencil computation but in essence, the runtimes differences will be very similar. From this, we can see that the big speedup can only be achieved if the computations can be inherently parallelised. Parallel Gradient Computation does not impact run times that much as the variables can't be horribly large. Hence it's wise to not call threads and just utilise the vector processes to parallelise the gradient computation and only use threads to distribute the computations in the graph across multiple threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.4",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
